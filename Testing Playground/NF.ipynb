{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(opt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    112\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mN_ITERS):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iter_ \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(iter_))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def w_1(z):\n",
    "    return torch.sin((2 * math.pi * z[:, 0]) / 4)\n",
    "\n",
    "\n",
    "def w_2(z):\n",
    "    return 3 * torch.exp(-.5 * ((z[:, 0] - 1) / .6) ** 2)\n",
    "\n",
    "\n",
    "def sigma(x):\n",
    "    return 1 / (1 + torch.exp(- x))\n",
    "\n",
    "\n",
    "def w_3(z):\n",
    "    return 3 * sigma((z[:, 0] - 1) / .3)\n",
    "\n",
    "\n",
    "def pot_1(z):\n",
    "    z_1, z_2 = z[:, 0], z[:, 1]\n",
    "    norm = torch.sqrt(z_1 ** 2 + z_2 ** 2)\n",
    "    outer_term_1 = .5 * ((norm - 2) / .4) ** 2\n",
    "    inner_term_1 = torch.exp((-.5 * ((z_1 - 2) / .6) ** 2))\n",
    "    inner_term_2 = torch.exp((-.5 * ((z_1 + 2) / .6) ** 2))\n",
    "    outer_term_2 = torch.log(inner_term_1 + inner_term_2 + 1e-7)\n",
    "    u = outer_term_1 - outer_term_2\n",
    "    return - u\n",
    "\n",
    "class PlanarFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    A single planar flow, computes T(x) and log(det(jac_T)))\n",
    "    \"\"\"\n",
    "    def __init__(self, D):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.u = nn.Parameter(torch.Tensor(1, D), requires_grad=True)\n",
    "        self.w = nn.Parameter(torch.Tensor(1, D), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.Tensor(1), requires_grad=True)\n",
    "        self.h = torch.tanh\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.w.data.uniform_(-0.01, 0.01)\n",
    "        self.b.data.uniform_(-0.01, 0.01)\n",
    "        self.u.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    def forward(self, z):\n",
    "        linear_term = torch.mm(z, self.w.T) + self.b\n",
    "        return z + self.u * self.h(linear_term)\n",
    "\n",
    "    def h_prime(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of tanh\n",
    "        \"\"\"\n",
    "        return (1 - self.h(x) ** 2)\n",
    "\n",
    "    def psi(self, z):\n",
    "        inner = torch.mm(z, self.w.T) + self.b\n",
    "        return self.h_prime(inner) * self.w\n",
    "\n",
    "    def log_det(self, z):\n",
    "        inner = 1 + torch.mm(self.psi(z), self.u.T)\n",
    "        return torch.log(torch.abs(inner))\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    A normalizing flow composed of a sequence of planar flows.\n",
    "    \"\"\"\n",
    "    def __init__(self, D, n_flows=2):\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.flows = nn.ModuleList(\n",
    "            [PlanarFlow(D) for _ in range(n_flows)])\n",
    "\n",
    "    def sample(self, base_samples):\n",
    "        \"\"\"\n",
    "        Transform samples from a simple base distribution\n",
    "        by passing them through a sequence of Planar flows.\n",
    "        \"\"\"\n",
    "        samples = base_samples\n",
    "        for flow in self.flows:\n",
    "            samples = flow(samples)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes and returns the sum of log_det_jacobians\n",
    "        and the transformed samples T(x).\n",
    "        \"\"\"\n",
    "        sum_log_det = 0\n",
    "        transformed_sample = x\n",
    "\n",
    "        for i in range(len(self.flows)):\n",
    "            log_det_i = (self.flows[i].log_det(transformed_sample))\n",
    "            sum_log_det += log_det_i\n",
    "            transformed_sample = self.flows[i](transformed_sample)\n",
    "\n",
    "        return transformed_sample, sum_log_det\n",
    "    \n",
    "    \n",
    "model = NormalizingFlow(2, 32)\n",
    "\n",
    "# RMSprop is what they used in renzende et al\n",
    "opt = torch.optim.RMSprop(\n",
    "    params=model.parameters(),\n",
    "    lr=0.005,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=1000)\n",
    "losses = []\n",
    "\n",
    "for iter_ in range(10):\n",
    "    if iter_ % 100 == 0:\n",
    "        print(\"Iteration {}\".format(iter_))\n",
    "\n",
    "    samples = Variable(random_normal_samples(100))\n",
    "\n",
    "    z_k, sum_log_det = model(samples)\n",
    "    log_p_x = target_density(z_k)\n",
    "\n",
    "    # Reverse KL since we can evaluate target density but can't sample\n",
    "    loss = (- sum_log_det - (log_p_x)).mean()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if iter_ % 100 == 0:\n",
    "        print(\"Loss {}\".format(loss.item())) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
